{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "with open('train_data.pickle', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "    num_records = len(train_data)\n",
    "with open('store_data.pickle', 'rb') as f:\n",
    "    store_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train datapoints:  844338\n",
      "46 41551\n",
      "[  0 109   1   0   0   0   0   7] 5961\n"
     ]
    }
   ],
   "source": [
    "def feature_list(record):\n",
    "    dt = datetime.strptime(record['Date'], '%Y-%m-%d')\n",
    "    store_index = int(record['Store'])\n",
    "    year = dt.year\n",
    "    month = dt.month\n",
    "    day = dt.day\n",
    "    day_of_week = int(record['DayOfWeek'])\n",
    "    try:\n",
    "        store_open = int(record['Open'])\n",
    "    except:\n",
    "        store_open = 1\n",
    "\n",
    "    promo = int(record['Promo'])\n",
    "\n",
    "    return [store_open,\n",
    "            store_index,\n",
    "            day_of_week,\n",
    "            promo,\n",
    "            year,\n",
    "            month,\n",
    "            day,\n",
    "            store_data[store_index - 1]['State']\n",
    "            ]\n",
    "\n",
    "\n",
    "train_data_X = []\n",
    "train_data_y = []\n",
    "\n",
    "for record in train_data:\n",
    "    if record['Sales'] != '0' and record['Open'] != '':\n",
    "        fl = feature_list(record)\n",
    "        train_data_X.append(fl)\n",
    "        train_data_y.append(int(record['Sales']))\n",
    "print(\"Number of train datapoints: \", len(train_data_y))\n",
    "\n",
    "print(min(train_data_y), max(train_data_y))\n",
    "\n",
    "full_X = train_data_X\n",
    "full_X = np.array(full_X)\n",
    "train_data_X = np.array(train_data_X)\n",
    "les = []\n",
    "for i in range(train_data_X.shape[1]):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(full_X[:, i])\n",
    "    les.append(le)\n",
    "    train_data_X[:, i] = le.transform(train_data_X[:, i])\n",
    "\n",
    "with open('les.pickle', 'wb') as f:\n",
    "    pickle.dump(les, f, -1)\n",
    "\n",
    "train_data_X = train_data_X.astype(int)\n",
    "train_data_y = np.array(train_data_y)\n",
    "\n",
    "with open('feature_train_data.pickle', 'wb') as f:\n",
    "    pickle.dump((train_data_X, train_data_y), f, -1)\n",
    "    print(train_data_X[0], train_data_y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples used for training: 200000\n",
      "Fitting NN_with_EntityEmbedding...\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 28s 142us/step - loss: 0.0142 - val_loss: 0.0119\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 27s 133us/step - loss: 0.0095 - val_loss: 0.0112\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 28s 141us/step - loss: 0.0088 - val_loss: 0.0109\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 34s 170us/step - loss: 0.0081 - val_loss: 0.0109\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 27s 136us/step - loss: 0.0077 - val_loss: 0.0100\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 28s 142us/step - loss: 0.0074 - val_loss: 0.0101\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 26s 132us/step - loss: 0.0072 - val_loss: 0.0097\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 27s 134us/step - loss: 0.0071 - val_loss: 0.0098\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 27s 136us/step - loss: 0.0069 - val_loss: 0.0094\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 28s 141us/step - loss: 0.0068 - val_loss: 0.0096\n",
      "Result on validation data:  0.10113687027966595\n",
      "Train on 200000 samples, validate on 84434 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 29s 143us/step - loss: 0.0140 - val_loss: 0.0112\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 28s 142us/step - loss: 0.0093 - val_loss: 0.0101\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 28s 141us/step - loss: 0.0083 - val_loss: 0.0101\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 28s 142us/step - loss: 0.0079 - val_loss: 0.0095\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 33s 166us/step - loss: 0.0076 - val_loss: 0.0096\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 27s 134us/step - loss: 0.0073 - val_loss: 0.0093\n",
      "Epoch 7/10\n",
      "  9856/200000 [>.............................] - ETA: 24s - loss: 0.0071"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(123)\n",
    "from models import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "train_ratio = 0.9\n",
    "shuffle_data = False\n",
    "one_hot_as_input = False\n",
    "embeddings_as_input = False\n",
    "save_embeddings = True\n",
    "saved_embeddings_fname = \"embeddings.pickle\"  # set save_embeddings to True to create this file\n",
    "\n",
    "f = open('feature_train_data.pickle', 'rb')\n",
    "(X, y) = pickle.load(f)\n",
    "\n",
    "num_records = len(X)\n",
    "train_size = int(train_ratio * num_records)\n",
    "\n",
    "if shuffle_data:\n",
    "    print(\"Using shuffled data\")\n",
    "    sh = numpy.arange(X.shape[0])\n",
    "    numpy.random.shuffle(sh)\n",
    "    X = X[sh]\n",
    "    y = y[sh]\n",
    "\n",
    "if embeddings_as_input:\n",
    "    print(\"Using learned embeddings as input\")\n",
    "    X = embed_features(X, saved_embeddings_fname)\n",
    "\n",
    "if one_hot_as_input:\n",
    "    print(\"Using one-hot encoding as input\")\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    enc.fit(X)\n",
    "    X = enc.transform(X)\n",
    "\n",
    "X_train = X[:train_size]\n",
    "X_val = X[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_val = y[train_size:]\n",
    "\n",
    "\n",
    "def sample(X, y, n):\n",
    "    '''random samples'''\n",
    "    num_row = X.shape[0]\n",
    "    indices = numpy.random.randint(num_row, size=n)\n",
    "    return X[indices, :], y[indices]\n",
    "\n",
    "\n",
    "X_train, y_train = sample(X_train, y_train, 200000)  # Simulate data sparsity\n",
    "print(\"Number of samples used for training: \" + str(y_train.shape[0]))\n",
    "\n",
    "models = []\n",
    "\n",
    "print(\"Fitting NN_with_EntityEmbedding...\")\n",
    "for i in range(5):\n",
    "    models.append(NN_with_EntityEmbedding(X_train, y_train, X_val, y_val))\n",
    "\n",
    "if save_embeddings:\n",
    "    model = models[0].model\n",
    "    store_embedding = model.get_layer('store_embedding').get_weights()[0]\n",
    "    dow_embedding = model.get_layer('dow_embedding').get_weights()[0]\n",
    "    year_embedding = model.get_layer('year_embedding').get_weights()[0]\n",
    "    month_embedding = model.get_layer('month_embedding').get_weights()[0]\n",
    "    day_embedding = model.get_layer('day_embedding').get_weights()[0]\n",
    "    german_states_embedding = model.get_layer('state_embedding').get_weights()[0]\n",
    "    with open(saved_embeddings_fname, 'wb') as f:\n",
    "        pickle.dump([store_embedding, dow_embedding, year_embedding,\n",
    "                     month_embedding, day_embedding, german_states_embedding], f, -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, X, y):\n",
    "    assert(min(y) > 0)\n",
    "    guessed_sales = numpy.array([model.guess(X) for model in models])\n",
    "    mean_sales = guessed_sales.mean(axis=0)\n",
    "    relative_err = numpy.absolute((y - mean_sales) / y)\n",
    "    result = numpy.sum(relative_err) / len(y)\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Evaluate combined models...\")\n",
    "print(\"Training error...\")\n",
    "r_train = evaluate_models(models, X_train, y_train)\n",
    "print(r_train)\n",
    "\n",
    "print(\"Validation error...\")\n",
    "r_val = evaluate_models(models, X_val, y_val)\n",
    "print(r_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
